{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DatabaseConnection:\n",
    "    def __init__(self):\n",
    "        # AWS RDS configuration\n",
    "        self.config = {\n",
    "            'host': 'redditdb.cbkuy486ce24.ap-south-1.rds.amazonaws.com',\n",
    "            'user': 'admin',     # Change this to your RDS master username (usually 'admin')\n",
    "            'password': 'admin123',  # Replace with your actual RDS password\n",
    "            'database': 'reddit01',\n",
    "            'port': 3306\n",
    "        }\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection to the database\"\"\"\n",
    "        try:\n",
    "            self.connection = mysql.connector.connect(**self.config)\n",
    "            if self.connection.is_connected():\n",
    "                db_info = self.connection.get_server_info()\n",
    "                self.cursor = self.connection.cursor(dictionary=True)\n",
    "                print(f\"Successfully connected to MySQL database version {db_info}\")\n",
    "                return True\n",
    "        except Error as e:\n",
    "            print(f\"Error connecting to MySQL Database: {e}\")\n",
    "            return False\n",
    "\n",
    "    def disconnect(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.connection and self.connection.is_connected():\n",
    "            if self.cursor:\n",
    "                self.cursor.close()\n",
    "            self.connection.close()\n",
    "            print(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reddit_data(db_connection, query, params=None):\n",
    "    \"\"\"Fetches data from the reddit_posts table.\n",
    "\n",
    "    Args:\n",
    "        db_connection: An instance of the DatabaseConnection class.\n",
    "        query: The SQL query string.\n",
    "        params: Optional parameters for the query (to prevent SQL injection).\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a row.\n",
    "        Returns None if there's an error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = db_connection.cursor\n",
    "        cursor.execute(query, params)\n",
    "        results = cursor.fetchall()\n",
    "        return results\n",
    "    except Error as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database version 8.0.39\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "db = DatabaseConnection()\n",
    "connect = db.connect()\n",
    "print(connect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_scraped_data_to_db(db, scraped_data):\n",
    "    try:\n",
    "        # Convert the scraped data to DataFrame\n",
    "        df = pd.DataFrame(scraped_data)\n",
    "        \n",
    "        # Convert created_utc to datetime if it's in unix timestamp format\n",
    "        if 'created_utc' in df.columns:\n",
    "            df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "        \n",
    "        # Prepare insert query\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO reddit_posts \n",
    "        (id, author, title, text, url, created_utc, score, num_comments, subreddit)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert DataFrame to list of tuples for batch insert\n",
    "        values = df.fillna('').apply(tuple, axis=1).tolist()\n",
    "        \n",
    "        # Batch insert records\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(values), batch_size):\n",
    "            batch = values[i:i + batch_size]\n",
    "            try:\n",
    "                db.cursor.executemany(insert_query, batch)\n",
    "                db.connection.commit()\n",
    "                print(f\"Inserted records {i} to {i + len(batch)}\")\n",
    "            except Error as e:\n",
    "                print(f\"Error inserting batch: {e}\")\n",
    "                db.connection.rollback()\n",
    "                \n",
    "        print(f\"Data import completed successfully! Total records inserted: {len(df)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during import: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your scraped_data is a list of dictionaries or similar structure\n",
    "# import_scraped_data_to_db(db, scraped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reddit_table(db):\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS reddit_posts (\n",
    "        id VARCHAR(255) PRIMARY KEY,\n",
    "        author VARCHAR(255) NOT NULL,\n",
    "        title TEXT NOT NULL,\n",
    "        text LONGTEXT,\n",
    "        url TEXT,\n",
    "        created_utc TIMESTAMP,\n",
    "        score INT DEFAULT 0,\n",
    "        num_comments INT DEFAULT 0,\n",
    "        subreddit VARCHAR(255) NOT NULL,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        db.cursor.execute(create_table_query)\n",
    "        db.connection.commit()\n",
    "        print(\"Table 'reddit_posts' created successfully!\")\n",
    "        \n",
    "        # Create indexes for better query performance\n",
    "        indexes = [\n",
    "            \"CREATE INDEX idx_author ON reddit_posts(author);\",\n",
    "            \"CREATE INDEX idx_subreddit ON reddit_posts(subreddit);\",\n",
    "            \"CREATE INDEX idx_created_utc ON reddit_posts(created_utc);\",\n",
    "            \"CREATE INDEX idx_score ON reddit_posts(score);\"\n",
    "        ]\n",
    "        \n",
    "        for index_query in indexes:\n",
    "            try:\n",
    "                db.cursor.execute(index_query)\n",
    "                db.connection.commit()\n",
    "            except Error as e:\n",
    "                # Skip if index already exists\n",
    "                if e.errno != 1061:  # 1061 is MySQL error for duplicate index\n",
    "                    print(f\"Error creating index: {e}\")\n",
    "                \n",
    "        print(\"Indexes created successfully!\")\n",
    "        \n",
    "    except Error as e:\n",
    "        print(f\"Error creating table: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.reddit_scraper import RedditScraper\n",
    "from src.settings import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 17:19:48,726 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 17:19:48,730 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 17:19:53,952 - RedditScraper - INFO - Collected 10 posts from r/python\n",
      "2024-12-16 17:19:57,601 - RedditScraper - INFO - Collected 10 posts from r/learnpython\n"
     ]
    }
   ],
   "source": [
    "scraper = RedditScraper()\n",
    "df = scraper.scrape_subreddit(\n",
    "        subreddit_names=Settings.DEFAULT_SUBREDDITS,\n",
    "        post_limit=Settings.DEFAULT_POST_LIMIT \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'rows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15216\\1494672845.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\projects\\test\\reddit-scrapper\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'rows'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted records 0 to 20\n",
      "Data import completed successfully! Total records inserted: 20\n"
     ]
    }
   ],
   "source": [
    "import_scraped_data_to_db(db=db,scraped_data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database version 8.0.39\n",
      "Retrieved 20 posts (limit was 20):\n",
      "{'id': '1heo2nz', 'author': 'Antique-Bowl-6384', 'title': 'Python comments', 'text': 'i found new comments:\\n```\\n\"comment\"\\n2\\n1.0\\n#comment\\n\"\"\"\\ncomment\\n\"\"\"\\n[\"comment\"]\\n(\"comment\")\\n{\"comment\"}\\n{\"comment\":\"okay\"}\\n```\\nif you dont use data types as variable you can use for comments like docstring', 'url': 'https://reddit.com/r/Python/comments/1heo2nz/python_comments/', 'created_utc': datetime.datetime(2024, 12, 15, 8, 7, 31), 'score': 0, 'num_comments': 7, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1heo8ps', 'author': 'Jaxondevs', 'title': \"PyGyat, What is everyone's thoughts on it\", 'text': \"[https://github.com/shamith09/pygyat](https://github.com/shamith09/pygyat)\\n\\nI saw this today and was wondering about everyone's thoughts\", 'url': 'https://reddit.com/r/Python/comments/1heo8ps/pygyat_what_is_everyones_thoughts_on_it/', 'created_utc': datetime.datetime(2024, 12, 15, 8, 20, 19), 'score': 0, 'num_comments': 4, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hewn0k', 'author': 'Automatic-Ad-2580', 'title': 'Documenting my First 30 Days Of Programming Python', 'text': 'Over the last 30 days i have been learning to programming and been doing a good job with consistently getting better and learning new things. Was just wondering if i can get anyones opinion  on what they think about my youtube channel i made to document my progress. If u do check i tout Please And Thank you.\\n\\n[https://www.youtube.com/watch?v=lh7\\\\_GZ6W6Jo](https://www.youtube.com/watch?v=lh7_GZ6W6Jo)', 'url': 'https://reddit.com/r/Python/comments/1hewn0k/documenting_my_first_30_days_of_programming_python/', 'created_utc': datetime.datetime(2024, 12, 15, 16, 49, 35), 'score': 0, 'num_comments': 4, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hez6qa', 'author': 'appinv', 'title': 'Summarized how the CIA writes Python', 'text': 'I have been going through Wikileaks and exploring Python usage within the CIA.\\n\\nThey have coding standards and write Python software with end-user guides.\\n\\nThey also have some curious ways of doing things, tests for example.\\n\\nThey also like to work in internet-disconnected environments.\\n\\nThey based their conventions on a modified Google Python Style Guide, with practical advice.\\n\\nCompiled [my findings](https://compileralchemy.substack.com/p/how-the-cia-writes-python).\\n\\n', 'url': 'https://reddit.com/r/Python/comments/1hez6qa/summarized_how_the_cia_writes_python/', 'created_utc': datetime.datetime(2024, 12, 15, 18, 43, 3), 'score': 552, 'num_comments': 72, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf0ci6', 'author': 'samamorgan', 'title': 'django-ngrok: One command to run your Django development server and tunnel to it with ngrok', 'text': \"Hi everyone!\\n\\nI work with webhooks quite a lot in my professional life, which means I'm almost always running ngrok alongside my Django development server. So I created a package that simplifies launching and configuring ngrok for use with Django.\\n\\n## What my project does\\nThis package introduces a new Django command, `runserver_ngrok`, that launches ngrok after the Django development server boots. The command simply extends the built-in `runserver` command to launch ngrok using `ngrok-python`, meaning you don't even have to install the ngrok binary.\\n\\n## Target audience\\nThis is intended for Django developers who, like me, also use ngrok in their daily workflows.\\n\\n## Comparison\\nI have yet to find a similar package that offers this functionality.\\n\\nWould love some feedback! Check it out on GitHub:\\n\\nhttps://github.com/samamorgan/django-ngrok\", 'url': 'https://reddit.com/r/Python/comments/1hf0ci6/djangongrok_one_command_to_run_your_django/', 'created_utc': datetime.datetime(2024, 12, 15, 19, 34, 39), 'score': 11, 'num_comments': 1, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf4i4v', 'author': 'TurbulentAd8020', 'title': 'GOAL: let the code focus on the core business logic and easy to maintain, pydantic-resolve', 'text': 'Last time my readme was failed, the highest comment is \"I do not understand what it does ...\", I learned from comments and revamped the doc a lot, hope this time it is more readable.\\n\\n\\nWhat My Project Does:\\n\\nhttps://github.com/allmonday/pydantic-resolve\\n\\npydantic-resolve is a lightweight wrapper library based on pydantic. It adds resolve and post methods to **pydantic** and **dataclass** objects.\\n\\nProblems to solve\\n\\nIf you have ever written similar code and felt unsatisfied, pydantic-resolve can come in handy.\\n\\n```python\\nstory_ids = [s.id for s in stories]\\ntasks = await get_all_tasks_by_story_ids(story_ids)\\n\\nstory_tasks = defaultdict(list)\\n\\nfor task in tasks:\\n    story_tasks[task.story_id].append(task)\\n\\nfor story in stories:\\n    tasks = story_tasks.get(story.id, [])\\n    story.tasks = tasks\\n    story.total_task_time = sum(task.time for task in tasks)\\n    story.total_done_tasks_time = sum(task.time for task in tasks if task.done)\\n    story.complex_result = ... calculation with many line\\n```\\n\\nThe problem is, this snippet mixed data fetching, traversal, variables and **business logic** together, which makes the core logic not easy to read.\\n\\npydantic-resolve can help **split them apart**, let developer focus on the core business logic, and leave other jobs to `Resolver().resolve`\\n\\nit introduced `resolve_method` for data fetching and `post_method` for extra midification after fetched.\\n\\nand the TaskLoader can be reused like a common component to load tasks by story_id\\n\\n```python\\nfrom pydantic_resolve import Resolver, LoaderDepend, build_list\\nfrom aiodataloader import DataLoader\\n\\n\\n# data fetching\\nclass TaskLoader(DataLoader):\\n    async def batch_load_fn(self, story_ids):\\n        tasks = await get_all_tasks_by_story_ids(story_ids)\\n        return build_list(tasks, story_ids, lambda t: t.story_id)\\n\\n# core business logics\\nclass Story(Base.Story):\\n    # fetch tasks\\n    tasks: List[Task] = []\\n    def resolve_tasks(self, loader=LoaderDepend(TaskLoader)):\\n        return loader.load(self.id)\\n\\n    # calc after fetched\\n    total_task_time: int = 0\\n    def post_total_task_time(self):\\n        return sum(task.time for task in self.tasks)\\n\\n    total_done_task_time: int = 0\\n    def post_total_done_task_time(self):\\n        return sum(task.time for task in self.tasks if task.done)\\n    \\n    complex_result: str = \\'\\'\\n    def post_complex_result(self):\\n        return  ... calculation with many line\\n  \\n# traversal and execute methods (runner)\\nawait Resolver().resolve(stories)\\n```\\n\\npydantic-resolve can easily be applied to more complicated scenarios, such as:\\n\\nA list of sprint, each sprint owns a list of story, each story owns a list of task, and do some modifications or calculations.\\n\\n```python\\n# data fetching\\nclass TaskLoader(DataLoader):\\n    async def batch_load_fn(self, story_ids):\\n        tasks = await get_all_tasks_by_story_ids(story_ids)\\n        return build_list(tasks, story_ids, lambda t: t.story_id)\\n\\nclass StoryLoader(DataLoader):\\n    async def batch_load_fn(self, sprint_ids):\\n        stories = await get_all_stories_by_sprint_ids(sprint_ids)\\n        return build_list(stories, sprint_ids, lambda t: t.sprint_id)\\n\\n# core business logic\\nclass Story(Base.Story):\\n    tasks: List[Task] = []\\n    def resolve_tasks(self, loader=LoaderDepend(TaskLoader)):\\n        return loader.load(self.id)\\n\\n    total_task_time: int = 0\\n    def post_total_task_time(self):\\n        return sum(task.time for task in self.tasks)\\n\\n    total_done_task_time: int = 0\\n    def post_total_done_task_time(self):\\n        return sum(task.time for task in self.tasks if task.done)\\n\\n\\nclass Sprint(Base.Sprint):\\n    stories: List[Story] = []\\n    def resolve_stories(self, loader=LoaderDepend(StoryLoader)):\\n        return loader.load(self.id)\\n    \\n    total_time: int = 0\\n    def post_total_time(self):\\n        return sum(story.total_task_time for story in self.stories)\\n\\n    total_done_time: int = 0\\n    def post_total_done_time(self):\\n        return sum(story.total_done_task_time for story in self.stories)\\n    \\n\\n# traversal and execute methods (runner)\\nawait Resolver().resolve(sprints)\\n```\\n\\nwhich equals to...\\n\\n```python\\nsprint_ids = [s.id for s in sprints]\\nstories = await get_all_stories_by_sprint_id(sprint_ids)\\n\\nstory_ids = [s.id for s in stories]\\ntasks = await get_all_tasks_by_story_ids(story_ids)\\n\\nsprint_stories = defaultdict(list)\\nstory_tasks = defaultdict(list)\\n\\nfor story in stories:\\n    sprint_stories[story.sprint_id].append(story)\\n\\nfor task in tasks:\\n    story_tasks[task.story_id].append(task)\\n\\nfor sprint in sprints:\\n    stories = sprint_stories.get(sprint.id, [])\\n    sprint.stories = stories\\n\\n    for story in stories:\\n        tasks = story_tasks.get(story.id, [])\\n        story.total_task_time = sum(task.time for task in tasks)\\n        story.total_done_task_time = sum(task.time for task in tasks if task.done)\\n\\n    sprint.total_time = sum(story.total_task_time for story in stories) \\n    sprint.total_done_time = sum(story.total_done_task_time for story in stories) \\n```\\n\\n> dataloader can be optimized by ORM relationship if the data can be join internally. (dataloader is a more universal way)\\n', 'url': 'https://reddit.com/r/Python/comments/1hf4i4v/goal_let_the_code_focus_on_the_core_business/', 'created_utc': datetime.datetime(2024, 12, 15, 22, 43, 31), 'score': 0, 'num_comments': 0, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf62db', 'author': 'AutoModerator', 'title': 'Monday Daily Thread: Project ideas!', 'text': '# Weekly Thread: Project Ideas ðŸ’¡\\n\\nWelcome to our weekly Project Ideas thread! Whether you\\'re a newbie looking for a first project or an expert seeking a new challenge, this is the place for you.\\n\\n## How it Works:\\n\\n1. **Suggest a Project**: Comment your project ideaâ€”be it beginner-friendly or advanced.\\n2. **Build & Share**: If you complete a project, reply to the original comment, share your experience, and attach your source code.\\n3. **Explore**: Looking for ideas? Check out Al Sweigart\\'s [\"The Big Book of Small Python Projects\"](https://www.amazon.com/Big-Book-Small-Python-Programming/dp/1718501242) for inspiration.\\n\\n## Guidelines:\\n\\n* Clearly state the difficulty level.\\n* Provide a brief description and, if possible, outline the tech stack.\\n* Feel free to link to tutorials or resources that might help.\\n\\n# Example Submissions:\\n\\n## Project Idea: Chatbot\\n\\n**Difficulty**: Intermediate\\n\\n**Tech Stack**: Python, NLP, Flask/FastAPI/Litestar \\n\\n**Description**: Create a chatbot that can answer FAQs for a website.\\n\\n**Resources**: [Building a Chatbot with Python](https://www.youtube.com/watch?v=a37BL0stIuM)\\n\\n# Project Idea: Weather Dashboard\\n\\n**Difficulty**: Beginner\\n\\n**Tech Stack**: HTML, CSS, JavaScript, API\\n\\n**Description**: Build a dashboard that displays real-time weather information using a weather API.\\n\\n**Resources**: [Weather API Tutorial](https://www.youtube.com/watch?v=9P5MY_2i7K8)\\n\\n## Project Idea: File Organizer\\n\\n**Difficulty**: Beginner\\n\\n**Tech Stack**: Python, File I/O\\n\\n**Description**: Create a script that organizes files in a directory into sub-folders based on file type.\\n\\n**Resources**: [Automate the Boring Stuff: Organizing Files](https://automatetheboringstuff.com/2e/chapter9/)\\n\\nLet\\'s help each other grow. Happy coding! ðŸŒŸ', 'url': 'https://reddit.com/r/Python/comments/1hf62db/monday_daily_thread_project_ideas/', 'created_utc': datetime.datetime(2024, 12, 16, 0, 0, 10), 'score': 7, 'num_comments': 1, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf6jen', 'author': 'failure2report', 'title': 'Lost with Python Automation!', 'text': \"Hello everyone,\\n\\n  \\nI am currently trying to build up my skill-sets to become a network engineer. I have decided to learn python i have watch a few basic videos about the syntax and variables. RN Ive just learned how to make calculators, very basic things. However, I am trying to find a way to focus my learning on automation specifically, and maybe machine learning. I just don't know how to navigate in the endless course there is in the internet. if anyone can help and guide me where i can find something that can help me learn that is bang for my buck and isn't too expensive. i don't really want to become an expert in python i just want to dip my toes and know how to build automation system and from there i will go to whatever i want to do with that. I really don't want to waste time on learning things that are irrelevant to my current goals. i would like to learn the ai, machine learning, and robotics side but not now.\\n\\n  \\ni really appreciate for anyone who helps out \", 'url': 'https://reddit.com/r/learnpython/comments/1hf6jen/lost_with_python_automation/', 'created_utc': datetime.datetime(2024, 12, 16, 0, 22, 26), 'score': 2, 'num_comments': 1, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf6qob', 'author': 'Usedtohaveabike', 'title': 'Pandas, how to filter results of a dataframe based on results present in another data frame', 'text': 'I have two dataframes and I am trying to find values  from the  dataframe named data that are also present in data\\\\_2\\n\\n  \\n`data = {`  \\n\\xa0\\xa0`\"NAME\": [\\'Chris\\',\\xa0\\'Sam\\',\\xa0\\'Kelly\\', \\'Owen\\', \\'Susan\\'],`  \\n\\xa0\\xa0`\"ID\":\\xa0[22,\\xa035,\\xa010, 3]`\\n\\n`\"ROOM NUMBER\" : [101, 203, 14, 310]`  \\n}\\n\\n\\n\\n`data_2 = {`  \\n\\xa0\\xa0`\"NAME\": [\\'Tim\\',\\xa0\\'Sam\\',\\xa0\\'Susan\\',`  \\n\\xa0\\xa0`\"ID\":\\xa0[7,\\xa035, 3]`\\n\\n`\"ROOM NUMBER\" : [101, 203, 310]`  \\n}\\n\\n\\n\\nI want to return Sam and Susan with their ID and ROOM NUMBER as well. I have tried the below approaches but they have not worked. \\n\\n\\n\\n\\n\\n`subset_df\\xa0=\\xa0data[data[\\'NAME\\'].isin(data_2[\\'NAME\\'])]`\\n\\n`subset_df = data.merge(data_2, left_on=\\'NAME\\', right_on=\\'NAME\\')[[\\'NAME\\', \\'ID\\', \\'ROOM NUMBER\\']]`\\n\\n\\xa0', 'url': 'https://reddit.com/r/learnpython/comments/1hf6qob/pandas_how_to_filter_results_of_a_dataframe_based/', 'created_utc': datetime.datetime(2024, 12, 16, 0, 32, 30), 'score': 2, 'num_comments': 1, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf6uem', 'author': 'adambarrack', 'title': 'Replicating the MATLAB Workspace in Python?', 'text': 'Hi experienced python users. I am here seeking your advice.\\n\\nINTRO/CONTEXT: I taught myself to code in MATLAB and R. I mostly use MATLAB because it does better with the larger array sizes I need for my research. I am trying to transfer over to Python to join the modern era. I know how to code for my purposes, but I am a novice to python, though I am learning quickly. \\n\\nTHE PROBLEM: The absence of a workspace bothers me. I am very used to monitoring defined variables and size of data structures in my workspace. I use it often to ensure my analysis code is doing what I want it to. Now that I donâ€™t have it, I realize I am actually fairly reliant on it. Is there something that can replicate this in Python? If not, are there any coding practices that help you guys keep track of these things?\\n\\nEdit (Pertinent Information): I am using Jupityr Notebooks within Pycharm.\\n\\nNote - Scientific View is great, but it doesnâ€™t give me the same basic information as a workspace as far as I can tell. I just want a list of defined variables and their sizes, maybe the ability to expand and view each one?\\n\\nSecondarily - is this a bad habit? I am self-taught, so I am definitely open to feedback.\\n', 'url': 'https://reddit.com/r/Python/comments/1hf6uem/replicating_the_matlab_workspace_in_python/', 'created_utc': datetime.datetime(2024, 12, 16, 0, 37, 39), 'score': 9, 'num_comments': 33, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf7094', 'author': 'adambarrack', 'title': 'Replicating the MATLAB Workspace in Python?', 'text': 'Hi experienced python users. I am here seeking your advice.\\n\\nINTRO/CONTEXT: I taught myself to code in MATLAB and R. I mostly use MATLAB because it does better with the larger array sizes I need for my research. I am trying to transfer over to Python to join the modern era. I know how to code for my purposes, but I am a novice to python, though I am learning quickly. \\n\\nTHE PROBLEM: The absence of a workspace bothers me. I am very used to monitoring defined variables and size of data structures in my workspace. I use it often to ensure my analysis code is doing what I want it to. Now that I donâ€™t have it, I realize I am actually fairly reliant on it. Is there something that can replicate this in Python? If not, are there any coding practices that help you guys keep track of these things?\\n\\nEdit for pertinent information: I am using Jupityr Notebooks in Pycharm\\n\\nNote - Scientific View is great, but it doesnâ€™t give me the same basic information as a workspace as far as I can tell. I just want a list of defined variables and their sizes, maybe the ability to expand and view each one?\\n\\nSecondarily - is this a bad habit? I am self-taught, so I am definitely open to feedback.\\n', 'url': 'https://reddit.com/r/learnpython/comments/1hf7094/replicating_the_matlab_workspace_in_python/', 'created_utc': datetime.datetime(2024, 12, 16, 0, 45, 49), 'score': 4, 'num_comments': 3, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf7xh4', 'author': 'vaibhavreads', 'title': \"anyone's doing Angela Yu's 100 days python bootcamp?\", 'text': 'I am currently on day 14 and just downloaded the resource. However, the files contain gibberish instead of the expected data. Can anyone help me?', 'url': 'https://reddit.com/r/learnpython/comments/1hf7xh4/anyones_doing_angela_yus_100_days_python_bootcamp/', 'created_utc': datetime.datetime(2024, 12, 16, 1, 32, 12), 'score': 5, 'num_comments': 18, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hf9crr', 'author': 'mouseVed', 'title': 'online course for intermediate learner', 'text': \"Hi,\\n\\nI've taken AP Computer Science Principles and got a 5 on the AP test, do you guys know any good online python courses for people who have some coding experience? I know how to use lists/arrays/etc and I've taken another course on Java.\\n\\n  \\nThank you!\", 'url': 'https://reddit.com/r/learnpython/comments/1hf9crr/online_course_for_intermediate_learner/', 'created_utc': datetime.datetime(2024, 12, 16, 2, 47, 20), 'score': 2, 'num_comments': 1, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hfaswv', 'author': 'Dependent_Chard_498', 'title': 'A Satirical \"Enterprise-Grade\" Birthday Wishing Bot', 'text': \"https://github.com/Shredmetal/Enterprise-grade-birthday-wisher-bot-AWS-lambda\\n\\n**What My Project Does**\\n\\nI wanted to close off 2024 with a meme project in the spirit of FizzBuzzEnterpriseEdition, so I massively overengineered a birthday wishing bot and covered it in 2024 tropes like shoehorning AI in there together with serverless cloud architecture.\\n\\nIncludes joke LICENSE and CODEOWNERS files.\\n\\nThe architecture is actually cost-efficient and I pay $0.00 per month (AWS has a remarkably generous free tier for Lambda).\\n\\nIt could be made more enterprise-grade with more design patterns and more unnecessarily complicated exception handling but it's December and nearly time for my vacation.\\n\\n**Target Audience**\\n\\nIt's a joke project, so I hope it's funny to some of you.\\n\\n**Comparison**\\n\\nIt's a joke project that doesn't solve a real problem. Can probably be compared with other satirical overengineering projects.\", 'url': 'https://reddit.com/r/Python/comments/1hfaswv/a_satirical_enterprisegrade_birthday_wishing_bot/', 'created_utc': datetime.datetime(2024, 12, 16, 4, 8, 13), 'score': 16, 'num_comments': 2, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hfbkdm', 'author': 'Nicknam0616', 'title': 'The code is extremely unstable. Please help me!', 'text': 'gist : [https://gist.github.com/Nicknam0616/369133256ccbc5f772c58ae31c835a5c](https://gist.github.com/Nicknam0616/369133256ccbc5f772c58ae31c835a5c)\\n\\nI have uploaded the code to a gist. The code is very unstable; it crashes once out of every ten runs. I tried to add the following code to the keyPressEvent method:\\n\\n`def keyPressEvent(self, event):`\\n\\n`print(f\"Key pressed: {event.key()}\")`\\n\\n`if event.key() == Qt.Key_P and event.modifiers() & Qt.ControlModifier:`\\n\\n`print(\"Ctrl + P pressed. Exiting the application.\")`\\n\\n`QApplication.quit()`\\n\\nHowever, after adding this, it crashes five out of ten times. Can anyone tell me why my code is so unstable?  \\n', 'url': 'https://reddit.com/r/learnpython/comments/1hfbkdm/the_code_is_extremely_unstable_please_help_me/', 'created_utc': datetime.datetime(2024, 12, 16, 4, 53, 16), 'score': 4, 'num_comments': 1, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hfc6wo', 'author': 'ImprovementAlive870', 'title': 'How to create dating app algorithm without filtering', 'text': 'I want to create a dating algorithm that will suggest people according to user interactions and behavior. If a user interacts with female users, then suggest females; if they start interacting with males, then show them males. If a user likes individuals from a specific country or ethnicity, show profiles from those categories without using explicit query filtering. What can I do to achieve this?', 'url': 'https://reddit.com/r/learnpython/comments/1hfc6wo/how_to_create_dating_app_algorithm_without/', 'created_utc': datetime.datetime(2024, 12, 16, 5, 30, 43), 'score': 6, 'num_comments': 7, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hffyuq', 'author': 'pleides101', 'title': 'SQLAlchemy question for polymorphic reference ', 'text': \"Hi. Started to learn python and am working on a small project to build a trading system. I took help from various coding AI assistants to build up the different pieces and got it to a working state. However I was using bare SQL queries for all my data management and although I tried to minimise possibilities of error and injections, it was still looking cumbersome. So I wanted to experiment with an ORM and tried sql alchemy. But due to how my table relationships are modelled Im having difficulty figuring out how to express them in a way sql alchemy can understand, none of the AI assistants have been able to help me.\\n\\nBasically it is a hierarchy of Strategy -> StrategySet -> Leg.\\nAnd the Strategy and Leg entities have indirect StopLoss and TrailingStopLoss relationships. I'm not using a dedicated column on the Strategy and Leg entities to reference the StopLoss and TrailingStopLoss as it wouldn't work out. A Strategy or Leg can have multiple stop losses or Trailing Stop losses defined and hence it would be a one to many relationship. \\nSo I modelled the StopLoss and TrailingStopLoss to hold a polymorphic reference called entity type. Depending on the entity type(Strategy or Leg) the entity id could hold reference to a Strategy or Leg id. So now whenever a new StopLoss or TrailingStopLoss is to be created I want to ensure the entity type is validated to be Strategy or Leg and then ensure the corresponding entity id actually exists in Strategy or Leg. \\nI was able to do all of this manually when I wrote the SQL queries myself but now having trouble modelling all of this in SQL alchemy. Any help or advice on what to do? Should I just create dedicated StopLoss and TrailingStopLoss entities for Strategy and Leg separately? I wanted to keep redundant columns or tables to a minimum.\\n\\n\\nHere's the complete code(not working) for more details in case you need it - https://gist.github.com/karthik448/cd5bafb8e4cd5e37d5dfdffe26643d76\\n\\n\\nThe only working code that any of the AIs could do was to add in a stop loss and trailing stop loss reference id into the strategy and leg entities and use the relationship and back-populates to add it in when a stop loss entry is created. This is obviously not going to work out for me as I want to have one to many relationship here.\\n\", 'url': 'https://reddit.com/r/learnpython/comments/1hffyuq/sqlalchemy_question_for_polymorphic_reference/', 'created_utc': datetime.datetime(2024, 12, 16, 10, 8, 32), 'score': 3, 'num_comments': 3, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hfgjj2', 'author': 'Particular_Young3797', 'title': 'Python Subprocess BlockingIOError', 'text': 'Hi Python developers,\\n\\nAnyone know about the issue. Please explain and how to solve it?\\n\\n\\n\\nwith sync\\\\_playwright() as p:  \\n  File \"/usr/local/lib/python3.11/site-packages/playwright/sync\\\\_api/\\\\_context\\\\_manager.py\", line 77, in \\\\_\\\\_enter\\\\_\\\\_  \\ndispatcher\\\\_fiber.switch()  \\n  File \"/usr/local/lib/python3.11/site-packages/playwright/sync\\\\_api/\\\\_context\\\\_manager.py\", line 56, in greenlet\\\\_main  \\nself.\\\\_loop.run\\\\_until\\\\_complete(self.\\\\_connection.run\\\\_as\\\\_sync())  \\n  File \"/usr/local/lib/python3.11/asyncio/base\\\\_events.py\", line 654, in run\\\\_until\\\\_complete  \\nreturn future.result()  \\n\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^  \\n  File \"/usr/local/lib/python3.11/site-packages/playwright/\\\\_impl/\\\\_connection.py\", line 263, in run\\\\_as\\\\_sync  \\nawait self.run()  \\n  File \"/usr/local/lib/python3.11/site-packages/playwright/\\\\_impl/\\\\_connection.py\", line 272, in run  \\nawait self.\\\\_transport.connect()  \\n  File \"/usr/local/lib/python3.11/site-packages/playwright/\\\\_impl/\\\\_transport.py\", line 133, in connect  \\nraise exc  \\n  File \"/usr/local/lib/python3.11/site-packages/playwright/\\\\_impl/\\\\_transport.py\", line 120, in connect  \\nself.\\\\_proc = await asyncio.create\\\\_subprocess\\\\_exec(  \\n\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^  \\n  File \"/usr/local/lib/python3.11/asyncio/subprocess.py\", line 223, in create\\\\_subprocess\\\\_exec  \\ntransport, protocol = await loop.subprocess\\\\_exec(  \\n\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^  \\n  File \"/usr/local/lib/python3.11/asyncio/base\\\\_events.py\", line 1708, in subprocess\\\\_exec  \\ntransport = await self.\\\\_make\\\\_subprocess\\\\_transport(  \\n\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^  \\n  File \"/usr/local/lib/python3.11/asyncio/unix\\\\_events.py\", line 207, in \\\\_make\\\\_subprocess\\\\_transport  \\ntransp = \\\\_UnixSubprocessTransport(self, protocol, args, shell,  \\n\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^  \\n  File \"/usr/local/lib/python3.11/asyncio/base\\\\_subprocess.py\", line 36, in \\\\_\\\\_init\\\\_\\\\_  \\nself.\\\\_start(args=args, shell=shell, stdin=stdin, stdout=stdout,  \\n  File \"/usr/local/lib/python3.11/asyncio/unix\\\\_events.py\", line 818, in \\\\_start  \\nself.\\\\_proc = subprocess.Popen(  \\n\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^  \\n  File \"/usr/local/lib/python3.11/subprocess.py\", line 1026, in \\\\_\\\\_init\\\\_\\\\_  \\nself.\\\\_execute\\\\_child(args, executable, preexec\\\\_fn, close\\\\_fds,  \\n  File \"/usr/local/lib/python3.11/subprocess.py\", line 1885, in \\\\_execute\\\\_child  \\n[self.pid](http://self.pid) = \\\\_fork\\\\_exec(  \\n\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^\\\\^  \\nBlockingIOError: \\\\[Errno 11\\\\] Resource temporarily unavailable  \\n\\\\[2024-12-14, 00:52:06 UTC\\\\] {base\\\\_events.py:1785} ERROR - Future exception was never retrieved  \\nfuture: <Future finished exception=BlockingIOError(11, \\'Resource temporarily unavailable\\')>  \\n  \\n', 'url': 'https://reddit.com/r/Python/comments/1hfgjj2/python_subprocess_blockingioerror/', 'created_utc': datetime.datetime(2024, 12, 16, 10, 51, 18), 'score': 0, 'num_comments': 0, 'subreddit': 'python', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hfgo0q', 'author': 'lol_isuck69', 'title': 'aiohttp corrupts the download for pillow. (With NamedTempFiles)', 'text': 'So I have a flow where I download a lot of images using aiohttp and then use Pillow to resize them. I am using NamedTemporaryFiles for file storage.\\n\\nI get errors like:\\n\\n    SyntaxError: broken PNG file (chunk b\\'\\\\x00\\\\x00\\\\x00\\\\x00\\')\\n    Error: Error processing image: broken PNG file (chunk b\\'\\\\x00\\\\x00\\\\x00\\\\x00\\')\\n\\nAnd\\n\\n    PIL.UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fe1d2aca950>\\n    Error: Error processing image: cannot identify image file <_io.BytesIO object at 0x7fe1d2acf540>\\n\\nI tried GPT, it said that use semaphores to limit concurrent download, which I used but this issue still persists. For example, when I have a list of 100 images to download, I am using 10 semaphores and this occurs for 2-3 images. Everything works fine with sequential download using requests. The images are stored in S3.\\n\\nCode I am using:\\n\\n    async def download_and_convert_image(url, output_path, file, session, semaphore):\\n        downloaded_path = await download_file_async(url, output_path, session, semaphore)\\n        if downloaded_path:\\n            image_data = None\\n            with open(downloaded_path, \\'rb\\') as image_file:\\n                    image_data = image_file.read()\\n            file[\\'image\\'] = await convert_image(image_data)\\n            \\n    async def download_file_async(url, file_path, session, semaphore):\\n        \"\"\"Download a file asynchronously with concurrency limit.\"\"\"\\n        try:\\n            async with semaphore:  # Wait for the semaphore to allow this task\\n                async with session.get(url) as response:\\n                    response.raise_for_status()  # Ensure the request is successful (status 200)\\n                    async with aiofiles.open(file_path, \\'wb\\') as file:\\n                        # Download in chunks\\n                        async for chunk in response.content.iter_chunked(4096):\\n                            await file.write(chunk)\\n            return file_path\\n        except Exception as e:\\n            print(f\"Failed to download {url} to {file_path}: {e}\", exc_info=True)\\n            return None\\n        \\n    async def convert_image(image_data):\\n        try:\\n            image = Image.open(BytesIO(image_data)) <--- Errors out here\\n    \\n            # Do something with image\\n    \\n        except Exception as e:\\n            print(f\"Error processing image: {e}\", exc_info=True)\\n            return None\\n    \\n    async def process_files(images_list, tmp_dir, session, semaphore_limit=10):\\n        tasks = []\\n        # Create a semaphore to limit the number of concurrent tasks\\n        semaphore = asyncio.Semaphore(semaphore_limit)\\n    \\n        for file in images_list:\\n            # Handle image files\\n            if any(file[\\'name\\'].lower().endswith(ext) for ext in IMG_EXTENSIONS):\\n                file_download_url = # generate S3 url\\n                if file_download_url:\\n                    output_path = f\"{tmp_dir}/{file[\\'name\\']}\"\\n                    tasks.append(download_and_convert_image(file_download_url, output_path, file, session, semaphore))\\n        # Run all tasks concurrently, with a limit on the number of concurrent downloads\\n        await asyncio.gather(*tasks)\\n        \\n    async def get_images_async(files_items, tmp_dir):\\n        async with aiohttp.ClientSession() as session:\\n            tasks = [process_files(files_item, tmp_dir, session) for files_item in files_items]\\n            await asyncio.gather(*tasks)\\n    \\n    def get_images(files_items, tmp_dir):\\n        \"\"\"Synchronous wrapper for the asynchronous method.\"\"\"\\n        asyncio.run(get_images_async(files_items, tmp_dir))', 'url': 'https://reddit.com/r/learnpython/comments/1hfgo0q/aiohttp_corrupts_the_download_for_pillow_with/', 'created_utc': datetime.datetime(2024, 12, 16, 11, 0, 30), 'score': 1, 'num_comments': 1, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "{'id': '1hfgs6x', 'author': 'Vriherre', 'title': 'Issue with pip in different python version', 'text': 'So i am running a environment with python 3.13 but i need 3.12 for a specific task and to install a package. When i try to switch versions i just use py -3.12 in the terminal. Here comes the issue when i now try to \\n\\n    py -m pip install\\n\\nit gives me the error:\\n\\n     File \"<stdin>\", line 1\\n        py -m pip install climate-indices\\n              ^^^\\n    SyntaxError: invalid syntax\\n\\n  \\nWhy is pip not working here? I thought it should work in different versions?\\n\\nThis error is also given for me for other commands like trying to get the pip version or when trying to switch back to python 3.13. \\n\\nWhat is the issue here?', 'url': 'https://reddit.com/r/learnpython/comments/1hfgs6x/issue_with_pip_in_different_python_version/', 'created_utc': datetime.datetime(2024, 12, 16, 11, 8, 19), 'score': 1, 'num_comments': 7, 'subreddit': 'learnpython', 'created_at': datetime.datetime(2024, 12, 16, 11, 52, 9), 'updated_at': datetime.datetime(2024, 12, 16, 11, 52, 9)}\n",
      "Database connection closed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "db_conn = DatabaseConnection()\n",
    "try:\n",
    "    if db_conn.connect():\n",
    "        limit = 20  # Set your desired limit\n",
    "\n",
    "        query = \"SELECT * FROM reddit_posts LIMIT %s;\" # Limit in the query\n",
    "        params = (limit,) # Limit passed as a parameter\n",
    "\n",
    "        all_posts = fetch_reddit_data(db_conn, query, params)\n",
    "        if all_posts:\n",
    "            print(f\"Retrieved {len(all_posts)} posts (limit was {limit}):\")\n",
    "            for post in all_posts:\n",
    "                print(post)\n",
    "        else:\n",
    "             print(\"No posts found or an error occurred.\") # Handle the case where no posts are found\n",
    "\n",
    "finally:  # Ensure the connection is closed even if errors occur\n",
    "    db_conn.disconnect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts_by_subreddit(db_connection, subreddit):\n",
    "    \"\"\"Fetches posts from a specific subreddit.\n",
    "\n",
    "    Args:\n",
    "        db_connection: An instance of the DatabaseConnection class.\n",
    "        subreddit: The name of the subreddit.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries (posts) or None if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"SELECT * FROM reddit_posts WHERE subreddit = %s;\"\n",
    "    params = (subreddit,)\n",
    "    posts = fetch_reddit_data(db_connection, query, params)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MySQL database version 8.0.39\n",
      "Posts from r/nonduality (limit 1):\n",
      "Id: 1hflin4\n",
      "Author: januszjt\n",
      "Title: \"If you begin to understand what you are, without trying to change it, then what you are undergoes a transformation.\" J Krishnamurti\n",
      "Text: So, all effort must cease, it is futile you cannot be improved what you already are. Understand means to stand under and let illusions fall away.\n",
      "Url: https://reddit.com/r/nonduality/comments/1hflin4/if_you_begin_to_understand_what_you_are_without/\n",
      "Created_utc: 2024-12-16 15:25:02\n",
      "Score: 7\n",
      "Num_comments: 0\n",
      "Subreddit: nonduality\n",
      "Created_at: 2024-12-16 15:56:08\n",
      "Updated_at: 2024-12-16 15:56:08\n",
      "--------------------\n",
      "Database connection closed\n"
     ]
    }
   ],
   "source": [
    "def get_posts_by_subreddit(db_connection, subreddit, limit):  # Added limit parameter\n",
    "    \"\"\"Fetches posts from a specific subreddit with a limit.\n",
    "\n",
    "    Args:\n",
    "        db_connection: An instance of the DatabaseConnection class.\n",
    "        subreddit: The name of the subreddit.\n",
    "        limit: The maximum number of posts to retrieve (default is 10).\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries (posts) or None if an error occurs.\n",
    "    \"\"\"\n",
    "\n",
    "    query = \"SELECT * FROM reddit_posts WHERE subreddit = %s LIMIT %s;\" # Limit added to query\n",
    "    params = (subreddit, limit) # Limit passed as a parameter\n",
    "    posts = fetch_reddit_data(db_connection, query, params)\n",
    "    return posts\n",
    "\n",
    "\n",
    "\n",
    "# Example usage (with limit):\n",
    "\n",
    "db_conn = DatabaseConnection()\n",
    "if db_conn.connect():\n",
    "    try:\n",
    "        target_subreddit = \"nonduality\"  # Or any subreddit you want\n",
    "        limit = 1 # Set your desired limit\n",
    "\n",
    "\n",
    "        subreddit_posts = get_posts_by_subreddit(db_conn, target_subreddit, limit)\n",
    "\n",
    "\n",
    "        if subreddit_posts:\n",
    "            print(f\"Posts from r/{target_subreddit} (limit {limit}):\")\n",
    "            for post in subreddit_posts:\n",
    "                for field, value in post.items(): # Iterate through all fields/values\n",
    "                    print(f\"{field.capitalize()}: {value}\") # Print field name and value\n",
    "                print(\"-\" * 20)  # Separator between posts\n",
    "        else:\n",
    "            print(f\"No posts found for r/{target_subreddit} or an error occurred.\")\n",
    "\n",
    "    finally:\n",
    "        db_conn.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'settings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreddit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RedditScraper\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maws_handler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatabaseConnection\n",
      "File \u001b[1;32md:\\projects\\test\\reddit-scrapper\\src\\reddit.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load environment variables\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'settings'"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.reddit import RedditScraper\n",
    "from src.settings import Settings\n",
    "from src.aws_handler import DatabaseConnection,get_posts_by_subreddit,fetch_reddit_data\n",
    "#subreddit input\n",
    "def get_list_from_input():\n",
    "    while True:  # Loop until valid input is received\n",
    "        try:\n",
    "            input_str = input(\"Enter list elements separated by commas: \")\n",
    "            my_list = [item.strip() for item in input_str.split(\",\")]\n",
    "            return my_list  # Return the list if successful\n",
    "        except ValueError: # This is usually not needed for split(\",\") but good practice if your separator might cause a ValueError with certain inputs.\n",
    "            print(\"Invalid input. Please enter comma-separated values.\")\n",
    "\n",
    "\n",
    "sub_reddit = get_list_from_input()\n",
    "post_limit=10\n",
    "months=3\n",
    "scraper=RedditScraper()\n",
    "df=scraper.scrape_subreddit(subreddit_names=sub_reddit,post_limit=post_limit,months=months)\n",
    "print(df)\n",
    "\n",
    "db = DatabaseConnection()\n",
    "connect=db.connect()\n",
    "print(connect)\n",
    "\n",
    "\n",
    "# add a button if append then do this \n",
    "append_data_to_db(db=db,df=df)\n",
    "'''2nd part'''\n",
    "# then to show the whole data do this \n",
    "\n",
    "db_conn = DatabaseConnection()\n",
    "try:\n",
    "    if db_conn.connect():\n",
    "        limit = df.len()  # Set your desired limit\n",
    "\n",
    "        query = \"SELECT * FROM reddit_posts LIMIT %s;\" # Limit in the query\n",
    "        params = (limit,) # Limit passed as a parameter\n",
    "\n",
    "        all_posts = fetch_reddit_data(db_conn, query, params)\n",
    "        if all_posts:\n",
    "            print(f\"Retrieved {len(all_posts)} posts (limit was {limit}):\")\n",
    "            for post in all_posts:\n",
    "                print(post)\n",
    "        else:\n",
    "             print(\"No posts found or an error occurred.\") # Handle the case where no posts are found\n",
    "\n",
    "finally:  # Ensure the connection is closed even if errors occur\n",
    "    db_conn.disconnect()\n",
    "\n",
    "#or show the data based upon the sub reddit\n",
    "#show the all subreddit options \n",
    "\n",
    "db_conn = DatabaseConnection()\n",
    "if db_conn.connect():\n",
    "    try:\n",
    "        target_subreddit = input(\"subreddit filter\")  # Or any subreddit you want show the available options\n",
    "        limit = 1 # Set your desired limit\n",
    "\n",
    "\n",
    "        subreddit_posts = get_posts_by_subreddit(db_conn, target_subreddit, limit)\n",
    "\n",
    "\n",
    "        if subreddit_posts:\n",
    "            print(f\"Posts from r/{target_subreddit} (limit {limit}):\")\n",
    "            for post in subreddit_posts:\n",
    "                for field, value in post.items(): # Iterate through all fields/values\n",
    "                    print(f\"{field.capitalize()}: {value}\") # Print field name and value\n",
    "                print(\"-\" * 20)  # Separator between posts\n",
    "        else:\n",
    "            print(f\"No posts found for r/{target_subreddit} or an error occurred.\")\n",
    "\n",
    "    finally:\n",
    "        db_conn.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,933 - RedditScraper - INFO - Attempting to create Reddit client\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:13:59,939 - RedditScraper - INFO - Reddit client successfully created and verified\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n",
      "2024-12-16 21:14:04,072 - RedditScraper - INFO - Collected 1 posts from r/python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                author                              title  \\\n",
      "0  1hfgjj2  Particular_Young3797  Python Subprocess BlockingIOError   \n",
      "\n",
      "                                                text  \\\n",
      "0  Hi Python developers,\\n\\nAnyone know about the...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://reddit.com/r/Python/comments/1hfgjj2/p...   \n",
      "\n",
      "                created_utc  score  num_comments subreddit  \n",
      "0 2024-12-16 10:51:18+00:00      0             0    python  \n",
      "Successfully connected to MySQL database version 8.0.39\n",
      "True\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'is_connected'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 26\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[43mconnect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m()\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# 1. Get column names from DataFrame\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'cursor'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconnect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_connected\u001b[49m():\n\u001b[0;32m     53\u001b[0m         cursor\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     54\u001b[0m         connect\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bool' object has no attribute 'is_connected'"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd  # Import pandas\n",
    "from src.reddit import RedditScraper\n",
    "from src.settings import Settings\n",
    "from src.aws_handler import DatabaseConnection\n",
    "\n",
    "# ... (your existing code)\n",
    "\n",
    "sub_reddit = get_list_from_input()\n",
    "post_limit = 1\n",
    "months = 3\n",
    "scraper = RedditScraper()\n",
    "df = scraper.scrape_subreddit(subreddit_names=sub_reddit, post_limit=post_limit, months=months)\n",
    "print(df)\n",
    "\n",
    "db = DatabaseConnection()\n",
    "connect = db.connect()\n",
    "print(connect)\n",
    "\n",
    "# --- Append DataFrame to database ---\n",
    "\n",
    "try:\n",
    "    cursor = connect.cursor()\n",
    "\n",
    "    # 1. Get column names from DataFrame\n",
    "    columns = \", \".join(df.columns)\n",
    "\n",
    "    # 2. Create placeholders for values\n",
    "    placeholders = \", \".join([\"%s\"] * len(df.columns))\n",
    "\n",
    "    # 3. SQL query to insert data (replace 'your_table_name' with your actual table name)\n",
    "    sql = f\"INSERT INTO your_table_name ({columns}) VALUES ({placeholders})\"\n",
    "\n",
    "    # 4. Convert DataFrame rows to tuples for insertion\n",
    "    data = [tuple(row) for row in df.itertuples(index=False)] # index=False to exclude the index\n",
    "\n",
    "    # 5. Execute the query with data\n",
    "    cursor.executemany(sql, data)\n",
    "\n",
    "\n",
    "    # 6. Commit the changes\n",
    "    connect.commit() \n",
    "    print(f\"{cursor.rowcount} rows inserted successfully into table\")\n",
    "\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"Error: {err}\")\n",
    "finally:\n",
    "    if connect.is_connected():\n",
    "        cursor.close()\n",
    "        connect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting batch: 1062 (23000): Duplicate entry '1hfgjj2' for key 'reddit_posts.PRIMARY'\n",
      "Successfully appended 1 records to reddit_posts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "append_data_to_db(db=db,df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_data_to_db(db, df):\n",
    "    \"\"\"\n",
    "    Append data to a database table\n",
    "    \n",
    "    Args:\n",
    "        db: Database connection object\n",
    "        df: Pandas DataFrame to append\n",
    "        table_name: Name of the table to append to\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare insert query\n",
    "        columns = df.columns.tolist()\n",
    "        placeholders = ', '.join(['%s'] * len(columns))\n",
    "        column_names = ', '.join(columns)\n",
    "        \n",
    "        insert_query = f\"\"\"\n",
    "        INSERT INTO {\"reddit_posts\"} ({column_names})\n",
    "        VALUES ({placeholders})\n",
    "        \"\"\"\n",
    "        \n",
    "        # Convert DataFrame to list of tuples\n",
    "        values = df.fillna('').to_records(index=False).tolist()\n",
    "        \n",
    "        # Batch insert\n",
    "        batch_size = 1000\n",
    "        for i in range(0, len(values), batch_size):\n",
    "            batch = values[i:i+batch_size]\n",
    "            \n",
    "            try:\n",
    "                db.cursor.executemany(insert_query, batch)\n",
    "                db.connection.commit()\n",
    "                print(f\"Inserted batch from {i} to {i+len(batch)}\")\n",
    "            except Exception as batch_error:\n",
    "                print(f\"Error inserting batch: {batch_error}\")\n",
    "                db.connection.rollback()\n",
    "        \n",
    "        print(f\"Successfully appended {len(values)} records to {\"reddit_posts\"}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during append: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'author', 'title', 'text', 'url', 'created_utc', 'score',\n",
       "       'num_comments', 'subreddit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error inserting batch: 'NoneType' object has no attribute 'executemany'\n",
      "Error during append: 'NoneType' object has no attribute 'rollback'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "append_data_to_db(db=db,df=df,table_name=\"reddit01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "def get_list_from_input():\n",
    "    while True:  # Loop until valid input is received\n",
    "        try:\n",
    "            input_str = input(\"Enter list elements separated by commas: \")\n",
    "            my_list = [item.strip() for item in input_str.split(\",\")]\n",
    "            return my_list  # Return the list if successful\n",
    "        except ValueError: # This is usually not needed for split(\",\") but good practice if your separator might cause a ValueError with certain inputs.\n",
    "            print(\"Invalid input. Please enter comma-separated values.\")\n",
    "\n",
    "\n",
    "my_list = get_list_from_input()\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connection object: <src.aws_handler.DatabaseConnection object at 0x000001F21439F4D0>\n",
      "Type of db: <class 'src.aws_handler.DatabaseConnection'>\n",
      "Connection methods: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'config', 'connect', 'connection', 'cursor', 'disconnect']\n"
     ]
    }
   ],
   "source": [
    "from src.aws_handler import DatabaseConnection, import_scraped_data_to_db\n",
    "from src.reddit import RedditScraper\n",
    "\n",
    "# Create database connection\n",
    "db = DatabaseConnection()\n",
    "\n",
    "# Print the type and value of db to see what's happening\n",
    "print(\"Database connection object:\", db)\n",
    "print(\"Type of db:\", type(db))\n",
    "\n",
    "# Check if the connection is valid\n",
    "if db is None:\n",
    "    print(\"Database connection failed!\")\n",
    "else:\n",
    "    try:\n",
    "        # Check if the connection has the necessary methods\n",
    "        print(\"Connection methods:\", dir(db))\n",
    "    except Exception as e:\n",
    "        print(\"Error accessing connection:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB_HOST: redditdb.cbkuy486ce24.ap-south-1.rds.amazonaws.com\n",
      "DB_NAME: reddit01\n",
      "DB_USER: admin\n",
      "Database connection successful!\n",
      "Cursor created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Debugging function to check database connection\n",
    "def debug_database_connection():\n",
    "    try:\n",
    "        # Print out environment variables for debugging\n",
    "        print(\"DB_HOST:\", os.getenv('DB_HOST'))\n",
    "        print(\"DB_NAME:\", os.getenv('DB_NAME'))\n",
    "        print(\"DB_USER:\", os.getenv('DB_USER'))\n",
    "        \n",
    "        # Attempt to establish a direct connection\n",
    "        connection = mysql.connector.connect(\n",
    "            host=os.getenv('DB_HOST'),\n",
    "            database=os.getenv('DB_NAME'),\n",
    "            user=os.getenv('DB_USER'),\n",
    "            password=os.getenv('DB_PASSWORD'),\n",
    "            port=os.getenv(\"port\")\n",
    "        )\n",
    "        \n",
    "        # Check if connection is successful\n",
    "        if connection.is_connected():\n",
    "            print(\"Database connection successful!\")\n",
    "            cursor = connection.cursor()\n",
    "            print(\"Cursor created successfully\")\n",
    "            \n",
    "            # Close connection\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Failed to connect to the database\")\n",
    "            return False\n",
    "    \n",
    "    except Error as e:\n",
    "        print(f\"Error connecting to MySQL database: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the diagnostic\n",
    "debug_database_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
